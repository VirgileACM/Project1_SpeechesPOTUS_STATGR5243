---
title: "Project 1: Inaugural Speeches of US Presidents"
runtime: shiny
output:
  html_document: default
  html_notebook: default
---

![US Capitol for Trump 2017](../figs/InaugDay.jpg)


#Step 0 - Install and load libraries

We make sure that all the required libraries are correctly installed and loaded. We will use mainly NLP libraries.

```{r, message=FALSE, warning=FALSE}
packages.used=c("tm", "wordcloud", "RColorBrewer", "dplyr", "tydytext", "qdap", 
                "qdapTools", "readr", "xlsx", "shiny", "ggplot2", "reshape2", "ggpmisc", 
                "RWeka", "lsa", "topicmodels")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
library(qdap)
library(qdapTools)
library(readr)
library(xlsx)
library(shiny)
library(ggplot2)
library(reshape2)
library(ggpmisc)
library(RWeka)
library(lsa)
library(topicmodels)

# for home-made functions
source("../lib/complexityFunc.R")
```


This notebook was prepared with the following environmental settings.

```{r}
print(R.version)
```

# Step 1 - Read in the speeches for Corpus Study

In this first step, we read and transfer all the speeches in our first data format: Corpus. The goal is to then apply a frequency approach to model each speech.

```{r}
folder.path="../data/InauguralSpeeches/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prex.out=substr(speeches, 6, nchar(speeches)-4)

ff.all<-Corpus(DirSource(folder.path))
```

#Step 2 - Text processing

See [Basic Text Mining in R](https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html) for a more comprehensive discussion. 

For the speeches, we remove extra white space, convert all letters to the lower case, remove [stop words](https://github.com/arc12/Text-Mining-Weak-Signals/wiki/Standard-set-of-english-stopwords), removed empty words due to formatting errors, and remove punctuation. Then we compute the [Document-Term Matrix (DTM)](https://en.wikipedia.org/wiki/Document-term_matrix).
Finally we compute the [Term-Frequency Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) weighted document-term matrices for all speeches. This allow us to find quantify the importance of each word in a text in comparaison of our Corpus.

```{r}
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)

tdm.all<-TermDocumentMatrix(ff.all)

tdm.tidy=tidy(tdm.all)

tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))

dtm <- DocumentTermMatrix(ff.all,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
ff.dtm=tidy(dtm)
```

#Step 3 - Read the speeches for general analysis

Our goal is to find an interesting time-pattern only from the provided data, and then explore it in the most straight-forward fashion. To do so, we need a uniform dataframe that contains all the information related to the speeches.Hence, once our Corpus processing is done, we conduct in parralel an analysis of the meta data of each speech. We reload the dataset and merge it with the two other provided files to obtain the name of President, his Party, date and number of words of each speech.

To merge the files, we procede to a few specific changes such as the name of the President (that differ for some between the files), or fill-in by hand some missing information. Also, we make important assumptions regarding the party. To carry-out an analysis across history, we need continuity. Therefore, based on information gathered online we assume Washington and Adams to be the equivalent of nowadays Democrats, the Democratic-Republican party and the Whig party to be Republicans.

Sources:
[Federalist Party](https://en.wikipedia.org/wiki/Federalist_Party)
[Democratic-Republican Party](https://en.wikipedia.org/wiki/Democratic-Republican_Party)
[Whig Party](https://en.wikipedia.org/wiki/Whig_Party_(United_States))

```{r, message=FALSE, warning=FALSE}
# Load the texts
texts = paste(folder.path,speeches,sep="")
ing_speeches = data.frame(File=prex.out, stringsAsFactors =F)
ing_speeches$Speech = apply(as.data.frame(texts,stringsAsFactors =F),1,FUN=function(x)read_file(x))

# Put name of president, term, party
xlfile = read.xlsx2(file='../data/InaugurationInfo.xlsx',sheetIndex = 1, stringsAsFactors=F)
xlfile$File = paste(xlfile$File, xlfile$Term, sep='-')
ing_file <- merge(ing_speeches, xlfile, by='File')
ing_file$Words[9] = 1433        # fill the last row since empty

# Load and process date
dates = read.table("../data/InauguationDates.txt", sep='\t', header=T, stringsAsFactors=F)
# account for differences between db
dates[22,1] ="Grover Cleveland - I"
dates[24,1] ="Grover Cleveland - II"
dates[11,1] = "James K. Polk"
dates[20,1] = "James Garfield"
dates[8,1] = "Martin van Buren"
dates[37,1] = "Richard Nixon"

dates_processed = data.frame(President=dates$PRESIDENT[dates$FIRST!=""],Term=1, Date=dates$FIRST[dates$FIRST!=""], stringsAsFactors=F)
dates_processed <- rbind(dates_processed, data.frame(President=dates$PRESIDENT[dates$SECOND!=""],Term=2, Date=dates$SECOND[dates$SECOND!=""], stringsAsFactors=F))
dates_processed <- rbind(dates_processed, data.frame(President=dates$PRESIDENT[dates$THIRD!=""],Term=3, Date=dates$THIRD[dates$THIRD!=""], stringsAsFactors=F))
dates_processed <- rbind(dates_processed, data.frame(President=dates$PRESIDENT[dates$FOURTH!=""],Term=4, Date=dates$FOURTH[dates$FOURTH!=""], stringsAsFactors=F))

ing_file <- merge(ing_file, dates_processed, by=c('President','Term'), all.x = TRUE)
ing_file$Year <- apply(as.data.frame(ing_file$Date,stringsAsFactors =F),1,FUN=function(x)as.numeric(substr(x,nchar(x)-3,nchar(x))))
ing_file$Words <- as.numeric(ing_file$Words) # transform words to integer

# Approximation of Party to make it comparable to more recent US president
ing_file$Party[ing_file$Party=="NA"] <- "Democratic" # George Washington
ing_file$Party[ing_file$Party=="Fedralist"] <- "Democratic" # John Adams
ing_file$Party[ing_file$Party=="Democratic-Republican Party"] <- "Republican"
ing_file$Party[ing_file$Party=="Whig"] <- "Republican"
```


#Step 4 - Interactive visualize important words in individual speeches

In order to find some meaningful hidden information across the history of Inaugural speeches of [POTUS](https://en.wikipedia.org/wiki/President_of_the_United_States), we can visualize the speeches using our TF-IDF matrices on a Wordcloud. For each speech, the words are displayed according their relative importance in the Corpus. We also cross the information with our meta data to display in blue the Democrats and in red the Republicans, as well as the year of each speech.


```{r, warning=FALSE}
d<-new.env()
d[["Republican"]] <- "Reds"; d[["Democratic"]] <- "Blues";

shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1',
                              speeches,
                              selected=speeches[20])),
        column(4, selectInput('speech2', 'Speech 2', speeches,
                              selected=speeches[9])),
        column(4, sliderInput('nwords', 'Number of words', 3,
                               min = 20, max = 200, value=100, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "600px")
      )
    ),

    server = function(input, output, session) {

      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(input$speech1)],
             dtm.count1=ff.dtm$count[ff.dtm$document==as.character(input$speech1)],
             dtm.term2=ff.dtm$term[ff.dtm$document==as.character(input$speech2)],
             dtm.count2=ff.dtm$count[ff.dtm$document==as.character(input$speech2)])
      })
      
      # find party and year of each speech in macro database
      
      charac <- reactive({
        s1 <- substr(input$speech1, 6, nchar(input$speech1)-4)
        s2 <- substr(input$speech2, 6, nchar(input$speech2)-4)
        list(P1=ing_file$Party[ing_file$File==s1],
             P2=ing_file$Party[ing_file$File==s2],
             Y1=ing_file$Year[ing_file$File==s1],
             Y2=ing_file$Year[ing_file$File==s2])
      })
      
      output$wordclouds <- renderPlot(height = 600, {
        #par(mfrow=c(2,2), mar = c(0, 0, 3, 0), heights=c(1,3))
        layout(matrix(c(1,2,3,4),2,2,byrow=TRUE), heights=c(1,15), TRUE)
        par(mar=c(0,0,0,0))
        frame()
        text(x=0.5, y=0.5, charac()$Y1, cex=2, font=2, col=ifelse(charac()$P1=="Democratic","deepskyblue2","firebrick2"))
        par(mar=c(0,0,0,0))
        frame()
        text(x=0.5, y=0.5, charac()$Y2, cex=2, font=2, col=ifelse(charac()$P2=="Democratic","deepskyblue2","firebrick2"))
        par(mar=c(0,0,0,0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=TRUE,
              random.color=FALSE,
              colors=brewer.pal(10,d[[charac()$P1]]),
            main=input$speech1)
        par(mar=c(0,0,0,0))
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=TRUE,
              random.color=FALSE,
              colors=brewer.pal(10,d[[charac()$P2]]), 
            main=input$speech2)
      })
    },

    options = list(height = 700)
)
```
Starting by comparing the inaugural speech of the 1st POTUS and the last one, one key difference is striking: the level of language. While George Washington`s most relevant words are **providential**, **immutable**, **impressions**, **qualifications** and **peculiarly**, Donal J. Trump`s top words are **America**, **Obama**, **dreams**, **everyone** and **dreams**. Comparing after Lincoln`s first speech with Obama`s, we observe the same difference: **clause**, **secede**, **case**, **minority** and **surrendered** against **icy**, **jobs**, **storms**, **winter** and **generation**.

Then multiple questions arise:

- Did the complexity of language of American presidents decreased during history?

- Are the Democrats better/worse/equivalent in term of lingustic than Republicans?

- How do President evolve during 2 terms or more?

- How to quantify the level of lingustic complexity?

The reponse to the last question is at the beginning the most important. For modelling this relationship, we decide to use classical techniques that has been proven to be robust for many types of text. The methods are [Coleman Liau](https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index),
[Automated Readability Index](https://en.wikipedia.org/wiki/Automated_readability_index) and  [Flesch_Kincaid](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests). They all quantify with a score the Reader's required level of education to understand most of the text. The score can then be translated to an equivalent in US Grades level. All 3 are formulas that were somehow fitted to some text-data, hence we proceed to the equivalent of a small regression ensemble-learning procedure where we take the average over the 3 learners. They also all rely on a frequentist approach where we use features such as the length of words, the length of sentences or the number of times a word is repeated. 
It is worth noting that the field of linguistic complexity is quite vast and interesting, and there obviously exist other methods. However, most of these [methods](http://images.pearsonassessments.com/images/tmrs/Word_Maturity_and_Text_Complexity_NCME.pdf) are proprietary and made for eduction; they also directly link the result to the reader, to achieve a better precision, but it is not possible for our dataset as we try to estimate an absolute index of readability.

# Step 5 - Computation of Readability Index

We now calculate the Readability index using a helper function. We also clean our datasets and prepare a few modifications for visualization purposes. Our main goal is here to concentrate as much as information on one plot to be able to visualize quickly different phenomenons.

```{r, message=FALSE, warning=FALSE}
ing_file$Speech[34] <- gsub("\u0097","",ing_file$Speech[34]) #avoid problems with exists()
# Calculation of Reading Index as the average of 3 calculated Index
ing_file$ReadIndex <- unlist(lapply(ing_file$Speech,FUN=ReadabilityIndexes))
# add abreviation for plotting
ing_file$Abrev <- unlist(lapply(ing_file$President,FUN=Abrev))
# squash words with sigmoid to display size of points accordingly
# -> choice of a squashing function to highlight the most extreme points and sig(x) is in [0,1]
ing_file$SigWords <- 20*sigmoid((ing_file$Words-mean(ing_file$Words))/sd(ing_file$Words)) # center and normalize before applying sigmoid
# create segment for president evolution per term
multiTermPresident <- c(ing_file$President[ing_file$Term>1], "Grover Cleveland - I")
df <- ing_file[ing_file$President %in% multiTermPresident,c("President" ,"Term", "Year", "ReadIndex")]
df$President[df$President=="Grover Cleveland - II"] <- "Grover Cleveland - I"  # to match Cleveland I and II
segments <- Agg_Year_ReadIndex(df)
# add segments for Franklin D. Roosevelt terms 3 and 4
n <- dim(segments)[1]
p <- "Franklin D. Roosevelt"
segments[(n+1),] <- c(filter(df, President==p, Term==2)$Year, filter(df, President==p, Term==2)$ReadIndex, filter(df, President==p, Term==3)$Year, filter(df, President==p, Term==3)$ReadIndex)
segments[(n+2),] <- c(filter(df, President==p, Term==3)$Year, filter(df, President==p, Term==3)$ReadIndex, filter(df, President==p, Term==4)$Year, filter(df, President==p, Term==4)$ReadIndex)
# create dummy variable for positive or negative evolution of ReadIndex
segments$signe <- (segments$RI2-segments$RI1)>=0
```

# Step 6 - Visualization of data

To start with, we simply plot the number of words according to the time, but by making the distinction between Republicans and Democrats President. We can observe that the turn-over of party is quite significant, with very rarely one party staying for more than 4 terms in a row. This allows us to assume a suffisant resampling of personalities and advisors of Presidents to keep a relatively low auto-correlation.

```{r, fig.height=6, fig.width=6, message=FALSE, warning=FALSE}
ggplot(ing_file, aes(Year,Words)) + geom_bar(aes(fill=Party),width=4, stat="identity") + scale_fill_manual(values=c("blue", "red")) + ggtitle("Evolution of Party representation across history") + theme(plot.title = element_text(hjust = 0.5))
```

After, we display the evolution of the Reading Index according to the time throughout American Presidential history. Higher score means a more difficult content, i.e. a more linguistic-complexified text. Without knowing how to interpret with precision the Y-axis, we can see direcly the down-trend that the President suffer from. In about 200 years, the average Readability Index has been divided by 2. 

With now, a more complex graph, we will answer most of the questions we asked ourselves before.

```{r, fig.height=6, fig.width=6, message=FALSE, warning=FALSE}
ggplot(ing_file, aes(Year,ReadIndex)) + geom_bar(aes(fill=Party),width=4, stat="identity") + scale_fill_manual(values=c("blue", "red")) + ggtitle("Evolution of Readability Index representation across history") + theme(plot.title = element_text(hjust = 0.5))
```

# Step 7 - Interactive Visualization of the evolution of the Readability Index across American Inaugural Speeches history (Shiny app)

This Shiny app is made to be discovered by the user to make her or him forge an enlighten opinion on the topic. Mostly, the app provides the ability to conduct differents kind of regression, on different timelines and for each party. Also, it is possible to display the evolution term per term of the president to see if they actually increased or decreased their linguistic level. To give a reference, a scale with US grade system is available. As a reminder, 7th grade is about 12 years old, 9th grade is 14 years old and college is about 18 year old.

```{r, warning=FALSE}
# home-made dictionnary to convert input choices to working methods
dict<-new.env()
dict[["Local Smooths"]] <- "loess";  dict[["Linear Model"]] <- "lm";
dict[["Polynomial order 2"]] <- "lm"; dict[["Polynomial order 3"]] <- "lm";

shinyApp(
    # render UI
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(3, selectInput('disp_party', label = 'Which party to display', choices = c("None","Democratic","Republican","Both"))),
        column(3, selectInput('reg_type', label = 'Type curve fitting', choices = c("Local Smooths","Linear Model","Polynomial order 2","Polynomial order 3"))),
        column(3, sliderInput('y1', 'Bound year 1', value=1789,
                               min = 1789, max = 2017, step = 4)),
        column(3, sliderInput('y2', 'Bound year 2', value=2017,
                               min = 1789, max = 2017, step = 4)),
        column(3, checkboxInput("disp_sizePoint", "Point size ~ nb of words", value = FALSE), checkboxInput("disp_words", "Display number of words", value = FALSE)),
        column(3, checkboxInput("disp_evol_term", "Display evolution president", value = FALSE), checkboxInput("disp_names", "Display names", value = FALSE)),
        column(3, checkboxInput("disp_levels", "Display grades level", value = FALSE))
      ),
      fluidRow(
        plotOutput('Figure', height = "600")
      )
    ),

    server = function(input, output, session) {

      # filter main data and segments data
      selectedData <- reactive({
        list(data=filter(ing_file, Year>=input$y1) %>% filter(Year<=input$y2),
             segPlus=filter(segments, Y1>=input$y1) %>% filter(Y2<=input$y2) %>% filter(signe==TRUE),
             segMinus=filter(segments, Y1>=input$y1) %>% filter(Y2<=input$y2) %>% filter(signe==FALSE))
      })
      
      # ractive expression for dictionnary
      method <- reactive({
        dict[[input$reg_type]]
      })
  
      # reactive expression for curve fitting (alternative to dictionnary)
      expr <- reactive({
        if (input$reg_type=="Polynomial order 2") "y~poly(x, 2, raw=T)"
        else if (input$reg_type=="Polynomial order 3") "y~poly(x, 3, raw=T)"
        else if (input$reg_type=="Exponential") "log(y)~x"
        else "y~x"
      })
      
      # main plot object
      output$Figure <- renderPlot(height = 600, {
                plot <- ggplot() + geom_smooth(selectedData()$data, mapping=aes(Year,ReadIndex), method=method(), formula=expr(), colour="black", level=0.9, size=2)  + scale_x_continuous(limits = c(input$y1, input$y2)) + ggtitle("Evolution of language complexity during POTUS inaugural speech")
     if (input$disp_evol_term){
        plot <- plot + geom_segment(data=selectedData()$segPlus, mapping=aes(x=Y1, xend=Y2, y=RI1, yend=RI2), size=1, color="green", arrow=arrow(length=unit(0.3,"cm"))) + geom_segment(data=selectedData()$segMinus, mapping=aes(x=Y1, xend=Y2, y=RI1, yend=RI2), size=1, color="orange", arrow=arrow(length=unit(0.3,"cm"))) + guides(size=FALSE)
     }
     # "sigmoid-scale" of points if asked to (choice of sigmoid function explained before)
     if (input$disp_sizePoint){
        plot <- plot + geom_point(selectedData()$data,mapping=aes(Year,ReadIndex,colour = factor(Party), size=SigWords)) + scale_colour_manual(values=c("blue", "red"), name="Party") + guides(size=FALSE)
     }
     else {
        plot <- plot + geom_point(selectedData()$data, mapping=aes(Year,ReadIndex, colour = factor(Party)), size=3) + scale_colour_manual(values=c("blue", "red"), name="Party")
     }
     # display name of presidents if asked to
     if (input$disp_names){
        plot <- plot + geom_text(data=selectedData()$data, mapping=aes(Year,ReadIndex,label=Abrev),hjust=0.5, vjust=-0.9, size=4.5)
     }
     # display number of words if asked to            
     if (input$disp_words){
        plot <- plot + geom_text(data=selectedData()$data, mapping=aes(Year,ReadIndex,label=Words),hjust=0.5, vjust=1.6, size=4.5)
     }
     # display US grade equivalent if asked to
     if (input$disp_levels){
        plot <- plot + geom_hline(color=c("red","orange","green"),yintercept=c(8,10,14)) + annotate(geom="text", label=c("7th grade","9th Grade","College"), x=c(input$y1,input$y1,input$y1), y=c(8,10,14), vjust=-1, fontface=2)
     }
     # display curve fitting equations if not local smoothing
     if (input$reg_type!="Local Smooths") {
        plot <- plot + stat_poly_eq(data=selectedData()$data, formula = expr(), eq.with.lhs = "italic(hat(y))~`=`~", aes(Year,ReadIndex,label = paste(..eq.label.., ..rr.label.., sep = "~~~")),parse = TRUE, label.x.npc = 0.75, label.y.npc = 0.97, size=5)
     }
     # curve fitting for party if asked to
     if (input$disp_party=="Both" || input$disp_party=="Democratic") {
        plot <- plot + geom_smooth(data=selectedData()$data[selectedData()$ data$Party=="Democratic",], aes(Year,ReadIndex), method=method(), formula = expr(), level=0, size=1, linetype="dashed", colour="blue")
       if (input$reg_type!="Local Smooths") {
         plot <- plot + stat_poly_eq(data=selectedData()$data[selectedData()$ data$Party=="Democratic",], formula = expr(), eq.with.lhs = "italic(hat(y))~`=`~", aes(Year,ReadIndex, label = paste(..eq.label.., ..rr.label.., sep = "~~~")),parse = TRUE, label.x.npc = 0.75, label.y.npc = 0.90, color="blue", size=5)
       }
     }
     if (input$disp_party=="Both" || input$disp_party=="Republican"){
       plot <- plot + geom_smooth(data=ing_file[selectedData()$data$Party=="Republican",], aes(Year,ReadIndex), method=method(), formula = expr(), level=0, size=1, linetype="dashed", colour="red")
       if (input$reg_type!="Local Smooths") {
         plot <- plot + stat_poly_eq(data=ing_file[selectedData()$data$Party=="Republican",], formula = expr(), eq.with.lhs = "italic(hat(y))~`=`~", aes(Year,ReadIndex, label = paste(..eq.label.., ..rr.label.., sep = "~~~")),parse = TRUE, label.x.npc = 0.75, label.y.npc = 0.83, color="red", size=5)
       }
     }
     # final theme
     plot <- plot + theme(legend.text = element_text(size = rel(1.3)), legend.title = element_text(size = rel(1.3)) ,axis.text = element_text(size = rel(1.2)), axis.title = element_text(size = rel(1.3)), plot.title = element_text(size=rel(1.5), hjust=0.5), legend.position = c(0.85,0.5), legend.background = element_rect(fill=alpha('blue', 0))) + labs(y="US Grades level")

    print(plot)
      })
    },
    options = list(height = 800)
)
```

The results are quite clear, the **level is decreasing at least linearly at a rate of about 5% a year**. Also, **being Democrats or Republicans does not help**. It appears that they both are very similar in term of level of language. During the 19th century, Republicans seem to have been less sophiticated than their colleagues Democrats, however, the 20th century seem to prove the opposite with poor prestations of F.Roosevelt or L.Johnson. The worst grade is given to Georges H.W.Bush with a technical level Kindergarten. This seem quite accurate as underline this [source](http://www.globalresearch.ca/the-inaugural-speech-of-a-us-president-history-wisdom-and-the-trump-presidency/5568582) which relates that during his inaugural speech, the President G.H.W. Bush compared freedom to a kite.

Also, it is interesting to look at the levels crossed during the 20th century. According to the data, **no President since 1910 has given an inaugural speech at a level above College**. On the other hand, only 4 Presidents have given a speech below College level until the 20th century. However, it is important to know that historically, the inaugural speech was not delivered to such an important crowd as now. It is only very recently that the inaugural speech became so popular due to communication tools available but also by being open to the public (historically it was given in the Congress). Nevertheless, we can observe that **in-between 1975 to today, the average level is about 8th grade**. The recent speech of D.J. Trump is also dragging the average down with an equivalent level of a 6th grader. 

Finally, another interesting pattern is the evolution of linguistic level for Presidents that had more than one term. Clearly, some President (or their personal writer) seem to have been more lazy than other on their speeches. During tension and then War time, the level of F. Roosevelt significantly dropped, whereas Obama improved his speech the second year while already above the average. One specific interesting fact, is that **Presidents for their second term tend suffer from mean-reversion**, a higher-than average speech the first term seems a good predictor for a below average second speech according to the data both from 19th and 20th century.


# Step 8 - Topic modelling as an alternative to regression

Now, as a tentative, we will try to find some correlation between the complexity of speech and the topics covered.

To do so, we will use a Tokenizer in Bigram (tokenize word and couple of consecutive words), that we will modelize again through a Term Document Matrix for test with [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis) a similar approach to PCA but more suitable for text-based material.

We compute 10 concepts based on LSA, and we try from the 10 most relevant words from each concept to identify the idea behind that concept. For information, LSA is based on a [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) that try to find the best space to project the words according to the singular values of the Term Document Matrix.

```{r}
# to compute faster NGramTokenizer by avoiding parrallelization that is poorly implemented
options(mc.cores=1)
NgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=2))

# garbage word that comes often in result of LSA, keep result clean
ff.all<-tm_map(ff.all, removeWords, c("us", "americaprevious", "upon", "shall", "roman", "texas", "ballot", "amid", "arrive"))

# computation of the TDM
tdmgram<-TermDocumentMatrix(ff.all, control = list(tokenize = NgramTokenizer, wordLengths = c(1, Inf) ))

# we choose arbitrary 10 topic for LSA to identify
dims <- 10
ls <- lsa(as.textmatrix(as.matrix(tdmgram)), dims=dims)
concepts.names <- c("Constitution","State","Values","Action","Union","Universal","People","America","Laws","Nation")
# Find the top #10 relevant word for each concept
concepts.top10 <- apply(ls$tk, 2, function(x)unique(names((sort(x, decreasing = T))))[1:10])
for (i in 1:dims) { writeLines(paste("\n","Concept",i,": ",concepts.names[i],"\n",sep=" "));print(concepts.top10[,i]) }

```

We decide for conclusion to visualize the evolution of the 10 identified topics according to the time. The findings are not that great. Across the history, the level went down quite rapidly, but the topics covered appear to have been the same. From the graphic, the only topic that has suffered is "State", which covers mostly part of speeches on the importance of America and its Institutions.
On the other end, "America"" seem to be a topic well developped over the past few years.

```{r}
# update with concepts
ls_df <- as.data.frame(ls$dk)
#names(ls_df) <- concepts.names
ing_final <- cbind(ing_file, ls$dk)
names(ing_final)[12:21] <- concepts.names
ing_final[12:21] <- apply(ing_final[12:21], 2 , FUN=function(x)(x-min(x))/(max(x)-min(x))) # normalization variable to [0,1] range
ing_final[12:21] <- t(apply(ing_final[12:21],1,function(x)cumsum(x)/max(cumsum(x))))

ggplot(data=ing_final, aes(x=Year)) + geom_ribbon(aes(ymin=0,ymax=Nation), fill="gold")+ geom_ribbon(aes(ymin=0,ymax=Laws), fill="darkorange") + geom_ribbon(aes(ymin=0,ymax=America), fill="firebrick2")  + geom_ribbon(aes(ymin=0,ymax=People), fill="firebrick4") + geom_ribbon(aes(ymin=0,ymax=Universal), fill="darkmagenta") + geom_ribbon(aes(ymin=0,ymax=Union), fill="darkorchid4") + geom_ribbon(aes(ymin=0,ymax=Action), fill="darkorchid1") + geom_ribbon(aes(ymin=0,ymax=Values), fill="dodgerblue3") + geom_ribbon(aes(ymin=0,ymax=State), fill="dodgerblue") + geom_ribbon(aes(ymin=0,ymax=Constitution), fill="deepskyblue") + ggtitle("vrvr") + annotate(geom="text", label=concepts.names, x=rep(1790,10), y=seq(0.05,0.90,length.out = 10), vjust=-1, fontface=2)+ggtitle("Evolution of concepts across history as a stacked-bar") + labs(y="Proportion importance of concept") + theme(plot.title = element_text(hjust=0.5))
```

In conclusion, the level of linguistic used by the President of the United States of America seem to have suffered a lot for the past 2 centuries. However, the topics and ideas exploited have apparently remain the same. This down-trend can be interpreted by a general decrease in linguistic in the population, not only the President of the United States, but also by a strong mediatization that brought the need to be more reachable to the people of America.

![US Capitol for Trump 2017](../figs/POTUS.jpg)

For more information on Linguistic Complexity and the history of the POTUS inaugural speeches, here are some interesting links:

[Education Approach to Linguistic Complexity Research](http://achievethecore.org/content/upload/nelson_perfetti_liben_measures_of_text_difficulty_research_ela.pdf)
[Inaugural addresses of past presidents](http://www.cbsnews.com/news/inaugural-addresses-of-past-presidents/)
[Trump Inaugural Speech against History](http://www.latimes.com/opinion/opinion-la/la-ol-trump-inauguration-kennedy-rosevelt-20170119-story.html)

